{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version: 3.6.7 (default, Dec  5 2018, 15:02:05) \n",
      "[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)]\n",
      "pandas version: 0.24.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import scipy as sp \n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold \n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import SparsePCA, TruncatedSVD, LatentDirichletAllocation, NMF\n",
    "\n",
    "import zipfile, json\n",
    "import datetime as dt \n",
    "from collections import Counter\n",
    "\n",
    "import sys, glob, os \n",
    "print('python version:', sys.version)\n",
    "print('pandas version:', pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_vec(pd_series, n_components = 5):\n",
    "    '''\n",
    "    input a pandas series, return a vectorize pandas dataframe \n",
    "    '''\n",
    "    col_name = pd_series.name\n",
    "    # Generate text features:\n",
    "    # Initialize decomposition methods:\n",
    "    print('generating features from: {}'.format(col_name))\n",
    "    svd_ = TruncatedSVD(n_components=n_components)#, random_state=1337)\n",
    "    nmf_ = NMF(n_components=n_components)#, random_state=1337)\n",
    "\n",
    "    tfidf_col = TfidfVectorizer().fit_transform(pd_series.values)\n",
    "\n",
    "    svd_col = svd_.fit_transform(tfidf_col)\n",
    "    svd_col = pd.DataFrame(svd_col)\n",
    "    svd_col = svd_col.add_prefix('SVD_{}_'.format(col_name))\n",
    "\n",
    "    nmf_col = nmf_.fit_transform(tfidf_col)\n",
    "    nmf_col = pd.DataFrame(nmf_col)\n",
    "    nmf_col = nmf_col.add_prefix('NMF_{}_'.format(col_name))\n",
    "\n",
    "    text_features = [svd_col, nmf_col]\n",
    "\n",
    "    # Combine all extracted features:\n",
    "    text_features = pd.concat(text_features, axis=1)\n",
    "\n",
    "    return text_features\n",
    "\n",
    "def text_vec_df(pd_dataframe, n_components = 5): \n",
    "    df = []\n",
    "    for col in pd_dataframe.columns: \n",
    "        df_temp = text_vec(pd_dataframe[col], n_components)\n",
    "        df.append(df_temp)\n",
    "    return pd.concat(df, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizer  \n",
    "Ref: https://www.kaggle.com/wrosinski/baselinemodeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "\n",
    "\n",
    "# FROM: https://www.kaggle.com/myltykritik/simple-lgbm-image-features\n",
    "\n",
    "# The following 3 functions have been taken from Ben Hamner's github repository\n",
    "# https://github.com/benhamner/Metrics\n",
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)\n",
    "\n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = quadratic_weighted_kappa(y, X_p)\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']\n",
    "    \n",
    "def rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmean(ary, key): \n",
    "    ''' \n",
    "    get mean value associated with 'key' from a iterable 'ary'\n",
    "    '''\n",
    "    return np.array([x.get(key, np.nan) for x in ary]).mean()\n",
    "\n",
    "def getsum(ary, key): \n",
    "    ''' \n",
    "    get sum value associated with 'key' from a iterable 'ary'\n",
    "    '''\n",
    "    return np.array([x.get(key, np.nan) for x in ary]).sum() \n",
    "\n",
    "def getjoin(ary, key): \n",
    "    '''\n",
    "    get str join associated with 'key' from a iterable 'ary\n",
    "    '''\n",
    "    return ' '.join([x.get(key, '') for x in ary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read basic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-13 18:10:01.687631 start reading data\n",
      "kernel running environment: local\n",
      "data path: ../../../data/PetFinder.my-Adoption-Prediction/\n",
      "train data shape: (14993, 24)\n",
      "test data shape: (3948, 23)\n",
      "full data shape: (18941, 24)\n",
      "2019-03-13 18:10:01.931484 finish reading data\n"
     ]
    }
   ],
   "source": [
    "print(dt.datetime.now(), 'start reading data')\n",
    "\n",
    "if os.path.isdir('../input') and not os.path.isfile('../input/test.zip'): \n",
    "    envx = 'remote' \n",
    "    dirc = '../input/'\n",
    "else: \n",
    "    envx = 'local'\n",
    "    dirc = '../../../data/PetFinder.my-Adoption-Prediction/'\n",
    "print('kernel running environment:', envx) \n",
    "print('data path:', dirc) \n",
    "\n",
    "train = pd.read_csv(dirc+'train/train.csv')\n",
    "test = pd.read_csv(dirc+'test/test.csv')\n",
    "sample_sub = pd.read_csv(dirc+'test/sample_submission.csv')\n",
    "\n",
    "print('train data shape:', train.shape)\n",
    "print('test data shape:', test.shape)\n",
    "\n",
    "data = pd.concat([train, test], sort=False).reset_index()\n",
    "data.drop('index', axis=1, inplace=True)\n",
    "fullpetid = data.PetID\n",
    "print('full data shape:', data.shape)\n",
    "\n",
    "breed_labels = pd.read_csv(dirc+'breed_labels.csv')\n",
    "color_labels = pd.read_csv(dirc+'color_labels.csv')\n",
    "state_labels = pd.read_csv(dirc+'state_labels.csv')\n",
    "\n",
    "print(dt.datetime.now(), 'finish reading data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Type         Name  Age  Breed1  Breed2  Gender  Color1  Color2  Color3  \\\n",
      "0     2       Nibble    3     299       0       1       1       7       0   \n",
      "1     2  No Name Yet    1     265       0       1       1       2       0   \n",
      "2     1       Brisco    1     307       0       1       2       7       0   \n",
      "3     1         Miko    4     307       0       2       1       2       0   \n",
      "4     1       Hunter    1     307       0       1       1       0       0   \n",
      "\n",
      "   MaturitySize  FurLength  Vaccinated  Dewormed  Sterilized  Health  \\\n",
      "0             1          1           2         2           2       1   \n",
      "1             2          2           3         3           3       1   \n",
      "2             2          2           1         1           2       1   \n",
      "3             2          1           1         1           2       1   \n",
      "4             2          1           2         2           2       1   \n",
      "\n",
      "   Quantity  Fee  State                         RescuerID  VideoAmt  \\\n",
      "0         1  100  41326  8480853f516546f6cf33aa88cd76c379         0   \n",
      "1         1    0  41401  3082c7125d8fb66f7dd4bff4192c8b14         0   \n",
      "2         1    0  41326  fa90fa5b1ee11c86938398b60abc32cb         0   \n",
      "3         1  150  41401  9238e4f44c71a75282e62f7136c6b240         0   \n",
      "4         1    0  41326  95481e953f8aed9ec3d16fc4509537e8         0   \n",
      "\n",
      "                                         Description      PetID  PhotoAmt  \\\n",
      "0  Nibble is a 3+ month old ball of cuteness. He ...  86e1089a3       1.0   \n",
      "1  I just found it alone yesterday near my apartm...  6296e909a       2.0   \n",
      "2  Their pregnant mother was dumped by her irresp...  3422e4906       7.0   \n",
      "3  Good guard dog, very alert, active, obedience ...  5842f1ff5       8.0   \n",
      "4  This handsome yet cute boy is up for adoption....  850a43f90       3.0   \n",
      "\n",
      "   AdoptionSpeed  \n",
      "0              2  \n",
      "1              0  \n",
      "2              3  \n",
      "3              2  \n",
      "4              2  \n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_columns', 200):\n",
    "    print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start parsing data from sentiment and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-13 18:10:02.117308 start getting sentiment\n",
      "train sentiment shape: (14442, 6)\n",
      "test sentiment shape (3815, 6)\n",
      "full sentiment shape (18257, 6)\n",
      "2019-03-13 18:10:07.401904 finish getting sentiment\n"
     ]
    }
   ],
   "source": [
    "print(dt.datetime.now(), 'start getting sentiment')\n",
    "sentiment_cols = ['PetID', 'docSentiMag', 'docSentiScore', 'fullMag', 'fullScore', 'fullentities']\n",
    "\n",
    "def get_sentiment(s): \n",
    "    '''\n",
    "    parse sentiment from a sentiment json file 's' \n",
    "    '''\n",
    "    docSentiMag = s['documentSentiment']['magnitude'] \n",
    "    docSentiScore = s['documentSentiment']['score']\n",
    "\n",
    "    mag_sco = [x['sentiment'] for x in s['sentences']]\n",
    "    \n",
    "    fullMag = getsum(mag_sco, 'magnitude')\n",
    "    fullScore = getsum(mag_sco, 'score')\n",
    "    \n",
    "    fullentities = getjoin(s['entities'], 'name')\n",
    "    \n",
    "    return [docSentiMag, docSentiScore, fullMag, fullScore, fullentities]\n",
    "\n",
    "def get_sentiment_f(myfile): \n",
    "    '''\n",
    "    forward a file name string to get_sentiment \n",
    "    '''\n",
    "    s = json.load(open(myfile))       \n",
    "    return [myfile[myfile.rfind('/')+1:-5], *get_sentiment(s)]\n",
    "\n",
    "def get_sentiment_zip(zipfilename):\n",
    "    sentiment_proc = np.asarray([\n",
    "        get_sentiment_f(myfile) for myfile in glob.glob(zipfilename)\n",
    "    ])\n",
    "    df_senti = pd.DataFrame(sentiment_proc, columns = sentiment_cols)  \n",
    "    df_senti['docSentiMag'] = df_senti['docSentiMag'].astype('float')\n",
    "    df_senti['docSentiScore'] = df_senti['docSentiScore'].astype('float')\n",
    "    df_senti['fullMag'] = df_senti['fullMag'].astype('float')\n",
    "    df_senti['fullScore'] = df_senti['fullScore'].astype('float')\n",
    "    return df_senti\n",
    "\n",
    "\n",
    "# get raw sentiment data from json files \n",
    "train_senti = get_sentiment_zip(dirc+'train_sentiment/*.json')\n",
    "test_senti = get_sentiment_zip(dirc+'test_sentiment/*.json')   \n",
    "\n",
    "# join \n",
    "data_senti = pd.concat([train_senti, test_senti], sort=False)\n",
    "\n",
    "'''\n",
    "# find missing PetID\n",
    "fullpetid_ct = Counter(fullpetid)\n",
    "data_senti_ct = Counter(data_senti.PetID)\n",
    "fullpetid_ct.subtract(data_senti_ct)\n",
    "missing_petid = list(+fullpetid_ct)\n",
    "\n",
    "# generate DataFrame associated with missied PetID \n",
    "temp = [[missing_petid[i], np.nan, np.nan, np.nan, np.nan, '<MISSING>'] \n",
    "        for i in range(len(missing_petid))]\n",
    "df_missing_senti = pd.DataFrame(temp, columns=sentiment_cols)\n",
    "\n",
    "data_senti = pd.concat([data_senti, df_missing_senti], axis=0, sort=False).\\\n",
    "    reset_index().drop('index', axis=1)\n",
    "\n",
    "vec_col = 'fullentities'\n",
    "\n",
    "data_senti_proc = text_vec(data_senti[vec_col], 5)\n",
    "data_senti = pd.concat([data_senti, data_senti_proc], axis=1)\n",
    "data_senti.drop(vec_col, axis=1, inplace=True)\n",
    "print('missing PetID sentiment shape', data_senti_proc.shape)\n",
    "'''\n",
    "\n",
    "print('train sentiment shape:', train_senti.shape)\n",
    "print('test sentiment shape', test_senti.shape)\n",
    "print('full sentiment shape', data_senti.shape)\n",
    "\n",
    "\n",
    "print(dt.datetime.now(), 'finish getting sentiment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PetID</th>\n",
       "      <th>docSentiMag</th>\n",
       "      <th>docSentiScore</th>\n",
       "      <th>fullMag</th>\n",
       "      <th>fullScore</th>\n",
       "      <th>fullentities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e6ff63097</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>pup adoption breed golden retriever mix pup Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14831f659</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>dog pity dog ss2 sea park area dog dog house i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>065906a54</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>owner care cat someone cat Malaysia rules rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03aae5768</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Looks whippet' breed greyhound family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e4dd90768</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.7</td>\n",
       "      <td>puppy walker hiking trail Melawati cutie cooki...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PetID  docSentiMag  docSentiScore  fullMag  fullScore  \\\n",
       "0  e6ff63097          1.2            0.3      1.0        1.0   \n",
       "1  14831f659          0.4           -0.4      0.4       -0.4   \n",
       "2  065906a54          1.7            0.2      1.5        1.5   \n",
       "3  03aae5768          1.1            0.5      1.0        1.0   \n",
       "4  e4dd90768          2.9            0.5      2.7        2.7   \n",
       "\n",
       "                                        fullentities  \n",
       "0  pup adoption breed golden retriever mix pup Ca...  \n",
       "1  dog pity dog ss2 sea park area dog dog house i...  \n",
       "2  owner care cat someone cat Malaysia rules rest...  \n",
       "3              Looks whippet' breed greyhound family  \n",
       "4  puppy walker hiking trail Melawati cutie cooki...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_senti.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-13 18:10:07.613393 start getting metadata\n",
      "train metadata shape: (58311, 7)\n",
      "test metadata shape (15040, 7)\n",
      "full metadata shape (73351, 7)\n",
      "2019-03-13 18:10:34.118140 finish getting metadata\n"
     ]
    }
   ],
   "source": [
    "print(dt.datetime.now(), 'start getting metadata')\n",
    "metadata_cols = ['PetID', \n",
    "                'labelAnnoScore', 'labelAnnoDesc', \n",
    "                'imagePropAnnoScore', 'imagePropAnnoPixelFrac', \n",
    "                'cropHintsAnnoConf', 'cropHintAnnoImport']\n",
    "\n",
    "def parse_metadata(s): \n",
    "    '''\n",
    "    parse metadata from a metadata json file 's' \n",
    "    '''\n",
    "    if 'labelAnnotations' in s: \n",
    "        s_label_Anno = s['labelAnnotations'][:int(0.3*len(s['labelAnnotations']))+1]\n",
    "        labelAnnoScore = getmean(s_label_Anno, 'score')\n",
    "        #labelAnnoDesc = s_label_Anno[0]['description']\n",
    "        labelAnnoDesc = getjoin(s_label_Anno, 'description')\n",
    "    else: \n",
    "        labelAnnoScore = np.nan \n",
    "        labelAnnoDesc = np.nan\n",
    "\n",
    "    if 'imagePropertiesAnnotation' in s:\n",
    "        s_ipa_dom_colors = s['imagePropertiesAnnotation']['dominantColors']['colors']\n",
    "        imagePropAnnoScore = getmean(s_ipa_dom_colors, 'score')\n",
    "        imagePropAnnoPixelFrac = getmean(s_ipa_dom_colors, 'pixelFraction')\n",
    "    else: \n",
    "        imagePropAnnoScore = np.nan \n",
    "        imagePropAnnoPixelFrac = np.nan\n",
    "\n",
    "    if 'cropHintsAnnotation' in s:\n",
    "        s_cHA = s['cropHintsAnnotation']['cropHints']\n",
    "        cropHintsAnnoConf = getmean(s_cHA, 'confidence')\n",
    "        cropHintAnnoImport = getmean(s_cHA, 'importanceFraction')\n",
    "    else: \n",
    "        cropHintsAnnoConf = np.nan \n",
    "        cropHintAnnoImport = np.nan\n",
    "\n",
    "    return [labelAnnoScore, labelAnnoDesc, \n",
    "              imagePropAnnoScore, imagePropAnnoPixelFrac, \n",
    "              cropHintsAnnoConf, cropHintAnnoImport]\n",
    "\n",
    "def parse_metadata_f(myfile): \n",
    "    s = json.load(open(myfile))\n",
    "    return [myfile[myfile.rfind('/')+1:myfile.rfind('-')], *parse_metadata(s)]\n",
    "\n",
    "def parse_metadata_zip(zipfilename): \n",
    "    metadata_proc = np.asarray([\n",
    "        parse_metadata_f(myfile) for myfile in glob.glob(zipfilename)\n",
    "    ])\n",
    "    df_metadata = pd.DataFrame(metadata_proc, columns = metadata_cols)\n",
    "    df_metadata['labelAnnoScore'] = df_metadata['labelAnnoScore'].astype('float')\n",
    "    df_metadata['imagePropAnnoScore'] = df_metadata['imagePropAnnoScore'].astype('float')\n",
    "    df_metadata['imagePropAnnoPixelFrac'] = df_metadata['imagePropAnnoPixelFrac'].astype('float')\n",
    "    df_metadata['cropHintsAnnoConf'] = df_metadata['cropHintsAnnoConf'].astype('float')\n",
    "    df_metadata['cropHintAnnoImport'] = df_metadata['cropHintAnnoImport'].astype('float')\n",
    "    return df_metadata\n",
    "\n",
    "train_metad = parse_metadata_zip(dirc+'train_metadata/*.json')\n",
    "test_metad = parse_metadata_zip(dirc+'test_metadata/*.json')\n",
    "data_metad = pd.concat([train_metad, test_metad], sort=False)\n",
    "\n",
    "\n",
    "print('train metadata shape:', train_metad.shape)\n",
    "print('test metadata shape', test_metad.shape)\n",
    "print('full metadata shape', data_metad.shape)  \n",
    "\n",
    "print(dt.datetime.now(), 'finish getting metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PetID</th>\n",
       "      <th>labelAnnoScore</th>\n",
       "      <th>labelAnnoDesc</th>\n",
       "      <th>imagePropAnnoScore</th>\n",
       "      <th>imagePropAnnoPixelFrac</th>\n",
       "      <th>cropHintsAnnoConf</th>\n",
       "      <th>cropHintAnnoImport</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e736f4022</td>\n",
       "      <td>0.946534</td>\n",
       "      <td>floor flooring tile</td>\n",
       "      <td>0.097666</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f861fe441</td>\n",
       "      <td>0.874389</td>\n",
       "      <td>dog breed dog street dog dog breed group</td>\n",
       "      <td>0.082835</td>\n",
       "      <td>0.074638</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6da1ee245</td>\n",
       "      <td>0.872424</td>\n",
       "      <td>cat small to medium sized cats cat like mammal...</td>\n",
       "      <td>0.090962</td>\n",
       "      <td>0.048281</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8f32c880e</td>\n",
       "      <td>0.961112</td>\n",
       "      <td>dog dog like mammal dog breed</td>\n",
       "      <td>0.079665</td>\n",
       "      <td>0.052886</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9ca31b395</td>\n",
       "      <td>0.917344</td>\n",
       "      <td>cat small to medium sized cats cat like mammal...</td>\n",
       "      <td>0.097187</td>\n",
       "      <td>0.091496</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PetID  labelAnnoScore  \\\n",
       "0  e736f4022        0.946534   \n",
       "1  f861fe441        0.874389   \n",
       "2  6da1ee245        0.872424   \n",
       "3  8f32c880e        0.961112   \n",
       "4  9ca31b395        0.917344   \n",
       "\n",
       "                                       labelAnnoDesc  imagePropAnnoScore  \\\n",
       "0                                floor flooring tile            0.097666   \n",
       "1           dog breed dog street dog dog breed group            0.082835   \n",
       "2  cat small to medium sized cats cat like mammal...            0.090962   \n",
       "3                      dog dog like mammal dog breed            0.079665   \n",
       "4  cat small to medium sized cats cat like mammal...            0.097187   \n",
       "\n",
       "   imagePropAnnoPixelFrac  cropHintsAnnoConf  cropHintAnnoImport  \n",
       "0                0.065789                0.8                 1.0  \n",
       "1                0.074638                0.8                 1.0  \n",
       "2                0.048281                0.8                 1.0  \n",
       "3                0.052886                0.8                 1.0  \n",
       "4                0.091496                0.8                 1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_metad.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group PetID from image metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18473, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PetID</th>\n",
       "      <th>labelAnnoScore_mean</th>\n",
       "      <th>labelAnnoScore_sum</th>\n",
       "      <th>imagePropAnnoScore_mean</th>\n",
       "      <th>imagePropAnnoScore_sum</th>\n",
       "      <th>imagePropAnnoPixelFrac_mean</th>\n",
       "      <th>imagePropAnnoPixelFrac_sum</th>\n",
       "      <th>cropHintsAnnoConf_mean</th>\n",
       "      <th>cropHintsAnnoConf_sum</th>\n",
       "      <th>cropHintAnnoImport_mean</th>\n",
       "      <th>cropHintAnnoImport_sum</th>\n",
       "      <th>labelAnnoDesc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0008c5398</td>\n",
       "      <td>0.916467</td>\n",
       "      <td>5.498802</td>\n",
       "      <td>0.071256</td>\n",
       "      <td>0.427536</td>\n",
       "      <td>0.050027</td>\n",
       "      <td>0.300160</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>cat small to medium sized cats cat like mammal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a290e4</td>\n",
       "      <td>0.930162</td>\n",
       "      <td>1.860325</td>\n",
       "      <td>0.080857</td>\n",
       "      <td>0.161713</td>\n",
       "      <td>0.057316</td>\n",
       "      <td>0.114633</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>dog dog breed dog like mammal dog breed group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000c21f80</td>\n",
       "      <td>0.910548</td>\n",
       "      <td>2.731643</td>\n",
       "      <td>0.070803</td>\n",
       "      <td>0.212409</td>\n",
       "      <td>0.054727</td>\n",
       "      <td>0.164181</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>cat small to medium sized cats cat like mammal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000fb9572</td>\n",
       "      <td>0.925330</td>\n",
       "      <td>5.551982</td>\n",
       "      <td>0.089756</td>\n",
       "      <td>0.538534</td>\n",
       "      <td>0.064844</td>\n",
       "      <td>0.389065</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>dog like mammal dog breed dog dog breed group ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0011d7c25</td>\n",
       "      <td>0.917093</td>\n",
       "      <td>2.751280</td>\n",
       "      <td>0.083866</td>\n",
       "      <td>0.251597</td>\n",
       "      <td>0.075841</td>\n",
       "      <td>0.227523</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>cat small to medium sized cats whiskers cat li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PetID  labelAnnoScore_mean  labelAnnoScore_sum  \\\n",
       "0  0008c5398             0.916467            5.498802   \n",
       "1  000a290e4             0.930162            1.860325   \n",
       "2  000c21f80             0.910548            2.731643   \n",
       "3  000fb9572             0.925330            5.551982   \n",
       "4  0011d7c25             0.917093            2.751280   \n",
       "\n",
       "   imagePropAnnoScore_mean  imagePropAnnoScore_sum  \\\n",
       "0                 0.071256                0.427536   \n",
       "1                 0.080857                0.161713   \n",
       "2                 0.070803                0.212409   \n",
       "3                 0.089756                0.538534   \n",
       "4                 0.083866                0.251597   \n",
       "\n",
       "   imagePropAnnoPixelFrac_mean  imagePropAnnoPixelFrac_sum  \\\n",
       "0                     0.050027                    0.300160   \n",
       "1                     0.057316                    0.114633   \n",
       "2                     0.054727                    0.164181   \n",
       "3                     0.064844                    0.389065   \n",
       "4                     0.075841                    0.227523   \n",
       "\n",
       "   cropHintsAnnoConf_mean  cropHintsAnnoConf_sum  cropHintAnnoImport_mean  \\\n",
       "0                     0.8                    4.8                      1.0   \n",
       "1                     0.8                    1.6                      1.0   \n",
       "2                     0.8                    2.4                      1.0   \n",
       "3                     0.8                    4.8                      1.0   \n",
       "4                     0.8                    2.4                      1.0   \n",
       "\n",
       "   cropHintAnnoImport_sum                                      labelAnnoDesc  \n",
       "0                     6.0  cat small to medium sized cats cat like mammal...  \n",
       "1                     2.0      dog dog breed dog like mammal dog breed group  \n",
       "2                     3.0  cat small to medium sized cats cat like mammal...  \n",
       "3                     6.0  dog like mammal dog breed dog dog breed group ...  \n",
       "4                     3.0  cat small to medium sized cats whiskers cat li...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get text columns, and join them \n",
    "#data_metad_obj = data_metad.loc[:, (data_metad.dtypes == 'object') | (data_metad.columns == 'PetID')].copy()\n",
    "#data_metad_obj = data_metad_obj.groupby('PetID').sum()\n",
    "data_metad_obj = data_metad.groupby('PetID').labelAnnoDesc.unique().apply(lambda x: ' '.join(x))\n",
    "\n",
    "# vectorize text columns \n",
    "#data_metad_obj_proc = text_vec(data_metad_obj, 5)\n",
    "data_metad_obj_proc = data_metad_obj.to_frame()\n",
    "\n",
    "# get numeric columns, add additional features: mean, sum \n",
    "data_metad_num = data_metad.loc[:, (data_metad.dtypes != 'object') | (data_metad.columns == 'PetID')].copy()\n",
    "for col in list(data_metad.columns[data_metad.dtypes != 'object']): \n",
    "    myimputer = SimpleImputer() \n",
    "    data_metad_num[col] = myimputer.fit_transform(data_metad_num[col].values.reshape(-1, 1))\n",
    "data_metad_num = data_metad_num.groupby('PetID').agg(['mean', 'sum']).reset_index()\n",
    "\n",
    "data_metad_num.columns = [\"_\".join(x) for x in data_metad_num.columns.ravel()]\n",
    "data_metad_num.rename(columns={'PetID_': 'PetID'}, inplace=True)\n",
    "\n",
    "# concat text columns and num columns \n",
    "#data_metad1 = pd.concat([data_metad_num, data_metad_obj_proc], axis=1)\n",
    "data_metad1 = pd.merge(data_metad_num, data_metad_obj_proc, how='left', on='PetID')\n",
    "print(data_metad1.shape)\n",
    "data_metad1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join and Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count RescuerID occurrences: \n",
    "(don't understand why, but this feature is important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescuer_count = data.groupby(['RescuerID'])['PetID'].count().reset_index()\n",
    "rescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n",
    "\n",
    "data_resc = pd.merge(data, rescuer_count, how='left', on='RescuerID')\n",
    "data_resc.drop('RescuerID', axis=1, inplace=True)\n",
    "\n",
    "#data_resc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge sentiment and metadata, vectorize text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating features from: Description\n",
      "generating features from: fullentities\n",
      "generating features from: labelAnnoDesc\n",
      "(18941, 65)\n"
     ]
    }
   ],
   "source": [
    "data_sm = pd.merge(data_resc, data_senti, how='left', on='PetID')\n",
    "data_sm = pd.merge(data_sm, data_metad1, how='left', on='PetID')\n",
    "\n",
    "str_cols = ['Description', 'fullentities', 'labelAnnoDesc']\n",
    "data_sm[str_cols] = data_sm[str_cols].fillna('<MISSING>')\n",
    "data_text_vec = text_vec_df(data_sm[str_cols], n_components=5)\n",
    "data_sm.drop(str_cols, axis=1, inplace=True)\n",
    "\n",
    "data_sm = pd.concat([data_sm, data_text_vec], axis=1)\n",
    "data_sm.drop(['Name', 'PetID'], axis=1, inplace=True) \n",
    "\n",
    "#data1 = data_sm\n",
    "#data1['PhotoAmt'] = data1.PhotoAmt.astype('int')\n",
    "\n",
    "print(data_sm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing numeric values? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impute missing values? False\n"
     ]
    }
   ],
   "source": [
    "data1 = data_sm.copy()\n",
    "\n",
    "impute_missing_values = False \n",
    "print('impute missing values?', impute_missing_values)\n",
    "\n",
    "if impute_missing_values: \n",
    "    print(dt.datetime.now(), 'start imputing')\n",
    "    \n",
    "    # Impute missing values, because Metadata or Sentiment is not complete \n",
    "    data1_na_columns = list(data1.columns[\\\n",
    "                                (data1.isna().sum() != 0) & (data1.dtypes != 'object') \\\n",
    "                                ].drop('AdoptionSpeed'))\n",
    "    print('impute numeric columns with nan:', data1_na_columns)\n",
    "    for col in data1_na_columns: \n",
    "        myimputer1 = SimpleImputer() \n",
    "        data1[col] = myimputer1.fit_transform(data1[col].values.reshape(-1, 1))\n",
    "        \n",
    "    print(dt.datetime.now(), 'finish merging data')\n",
    "\n",
    "# astype to int \n",
    "data1['PhotoAmt'] = data1.PhotoAmt.astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>...</th>\n",
       "      <th>SVD_labelAnnoDesc_0</th>\n",
       "      <th>SVD_labelAnnoDesc_1</th>\n",
       "      <th>SVD_labelAnnoDesc_2</th>\n",
       "      <th>SVD_labelAnnoDesc_3</th>\n",
       "      <th>SVD_labelAnnoDesc_4</th>\n",
       "      <th>NMF_labelAnnoDesc_0</th>\n",
       "      <th>NMF_labelAnnoDesc_1</th>\n",
       "      <th>NMF_labelAnnoDesc_2</th>\n",
       "      <th>NMF_labelAnnoDesc_3</th>\n",
       "      <th>NMF_labelAnnoDesc_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271465</td>\n",
       "      <td>0.730297</td>\n",
       "      <td>3.018277e-07</td>\n",
       "      <td>-0.043794</td>\n",
       "      <td>-0.105300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.004518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370153</td>\n",
       "      <td>0.905112</td>\n",
       "      <td>4.227043e-07</td>\n",
       "      <td>-0.001500</td>\n",
       "      <td>0.092516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.055519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938243</td>\n",
       "      <td>-0.340000</td>\n",
       "      <td>9.423154e-08</td>\n",
       "      <td>-0.027664</td>\n",
       "      <td>-0.007926</td>\n",
       "      <td>0.098023</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.934383</td>\n",
       "      <td>-0.350260</td>\n",
       "      <td>-2.939679e-09</td>\n",
       "      <td>-0.027566</td>\n",
       "      <td>0.012194</td>\n",
       "      <td>0.098066</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465788</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>-7.014057e-07</td>\n",
       "      <td>-0.013312</td>\n",
       "      <td>-0.054549</td>\n",
       "      <td>0.047593</td>\n",
       "      <td>0.003383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Age  Breed1  Breed2  Gender  Color1  Color2  Color3  MaturitySize  \\\n",
       "0     2    3     299       0       1       1       7       0             1   \n",
       "1     2    1     265       0       1       1       2       0             2   \n",
       "2     1    1     307       0       1       2       7       0             2   \n",
       "3     1    4     307       0       2       1       2       0             2   \n",
       "4     1    1     307       0       1       1       0       0             2   \n",
       "\n",
       "   FurLength  ...  SVD_labelAnnoDesc_0  SVD_labelAnnoDesc_1  \\\n",
       "0          1  ...             0.271465             0.730297   \n",
       "1          2  ...             0.370153             0.905112   \n",
       "2          2  ...             0.938243            -0.340000   \n",
       "3          1  ...             0.934383            -0.350260   \n",
       "4          1  ...             0.465788            -0.142857   \n",
       "\n",
       "   SVD_labelAnnoDesc_2  SVD_labelAnnoDesc_3  SVD_labelAnnoDesc_4  \\\n",
       "0         3.018277e-07            -0.043794            -0.105300   \n",
       "1         4.227043e-07            -0.001500             0.092516   \n",
       "2         9.423154e-08            -0.027664            -0.007926   \n",
       "3        -2.939679e-09            -0.027566             0.012194   \n",
       "4        -7.014057e-07            -0.013312            -0.054549   \n",
       "\n",
       "   NMF_labelAnnoDesc_0  NMF_labelAnnoDesc_1  NMF_labelAnnoDesc_2  \\\n",
       "0             0.000000             0.090134                  0.0   \n",
       "1             0.000000             0.099664                  0.0   \n",
       "2             0.098023             0.000766                  0.0   \n",
       "3             0.098066             0.000000                  0.0   \n",
       "4             0.047593             0.003383                  0.0   \n",
       "\n",
       "   NMF_labelAnnoDesc_3  NMF_labelAnnoDesc_4  \n",
       "0             0.000144             0.004518  \n",
       "1             0.000176             0.055519  \n",
       "2             0.000000             0.000000  \n",
       "3             0.000000             0.000000  \n",
       "4             0.000458             0.000000  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dummies for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-13 18:10:50.858048 start processing data\n",
      "get dummies for categorica data? False\n",
      "change 'not sure' in Vaccinated, Sterilized, Dewormed to No? False\n",
      "(18941, 65)\n",
      "2019-03-13 18:10:50.878643 finish processing data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>...</th>\n",
       "      <th>SVD_labelAnnoDesc_0</th>\n",
       "      <th>SVD_labelAnnoDesc_1</th>\n",
       "      <th>SVD_labelAnnoDesc_2</th>\n",
       "      <th>SVD_labelAnnoDesc_3</th>\n",
       "      <th>SVD_labelAnnoDesc_4</th>\n",
       "      <th>NMF_labelAnnoDesc_0</th>\n",
       "      <th>NMF_labelAnnoDesc_1</th>\n",
       "      <th>NMF_labelAnnoDesc_2</th>\n",
       "      <th>NMF_labelAnnoDesc_3</th>\n",
       "      <th>NMF_labelAnnoDesc_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271465</td>\n",
       "      <td>0.730297</td>\n",
       "      <td>3.018277e-07</td>\n",
       "      <td>-0.043794</td>\n",
       "      <td>-0.105300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.004518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370153</td>\n",
       "      <td>0.905112</td>\n",
       "      <td>4.227043e-07</td>\n",
       "      <td>-0.001500</td>\n",
       "      <td>0.092516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.055519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938243</td>\n",
       "      <td>-0.340000</td>\n",
       "      <td>9.423154e-08</td>\n",
       "      <td>-0.027664</td>\n",
       "      <td>-0.007926</td>\n",
       "      <td>0.098023</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.934383</td>\n",
       "      <td>-0.350260</td>\n",
       "      <td>-2.939679e-09</td>\n",
       "      <td>-0.027566</td>\n",
       "      <td>0.012194</td>\n",
       "      <td>0.098066</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465788</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>-7.014057e-07</td>\n",
       "      <td>-0.013312</td>\n",
       "      <td>-0.054549</td>\n",
       "      <td>0.047593</td>\n",
       "      <td>0.003383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Age  Breed1  Breed2  Gender  Color1  Color2  Color3  MaturitySize  \\\n",
       "0     2    3     299       0       1       1       7       0             1   \n",
       "1     2    1     265       0       1       1       2       0             2   \n",
       "2     1    1     307       0       1       2       7       0             2   \n",
       "3     1    4     307       0       2       1       2       0             2   \n",
       "4     1    1     307       0       1       1       0       0             2   \n",
       "\n",
       "   FurLength  ...  SVD_labelAnnoDesc_0  SVD_labelAnnoDesc_1  \\\n",
       "0          1  ...             0.271465             0.730297   \n",
       "1          2  ...             0.370153             0.905112   \n",
       "2          2  ...             0.938243            -0.340000   \n",
       "3          1  ...             0.934383            -0.350260   \n",
       "4          1  ...             0.465788            -0.142857   \n",
       "\n",
       "   SVD_labelAnnoDesc_2  SVD_labelAnnoDesc_3  SVD_labelAnnoDesc_4  \\\n",
       "0         3.018277e-07            -0.043794            -0.105300   \n",
       "1         4.227043e-07            -0.001500             0.092516   \n",
       "2         9.423154e-08            -0.027664            -0.007926   \n",
       "3        -2.939679e-09            -0.027566             0.012194   \n",
       "4        -7.014057e-07            -0.013312            -0.054549   \n",
       "\n",
       "   NMF_labelAnnoDesc_0  NMF_labelAnnoDesc_1  NMF_labelAnnoDesc_2  \\\n",
       "0             0.000000             0.090134                  0.0   \n",
       "1             0.000000             0.099664                  0.0   \n",
       "2             0.098023             0.000766                  0.0   \n",
       "3             0.098066             0.000000                  0.0   \n",
       "4             0.047593             0.003383                  0.0   \n",
       "\n",
       "   NMF_labelAnnoDesc_3  NMF_labelAnnoDesc_4  \n",
       "0             0.000144             0.004518  \n",
       "1             0.000176             0.055519  \n",
       "2             0.000000             0.000000  \n",
       "3             0.000000             0.000000  \n",
       "4             0.000458             0.000000  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dt.datetime.now(), 'start processing data')\n",
    "\n",
    "data2 = data1.copy() \n",
    "\n",
    "# It seems like NOT getting dummies would give better results :( \n",
    "# get_dummies \n",
    "get_dummies_for_categorical = False \n",
    "print('get dummies for categorica data?', get_dummies_for_categorical)\n",
    "\n",
    "if get_dummies_for_categorical: \n",
    "    col_dummied = ['Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3', 'State']\n",
    "    data1_dummies = pd.get_dummies(data1[col_dummied].astype('object'))\n",
    "    data2 = pd.concat([data1, data1_dummies], axis=1)\n",
    "    data2.drop(col_dummied, axis=1, inplace=True)\n",
    "\n",
    "# 'Not Sure' in Vaccinated, Sterilized, or Dewormed -> 'No' \n",
    "vac_ster_deworm_impute = False \n",
    "print('change \\'not sure\\' in Vaccinated, Sterilized, Dewormed to No?', \n",
    "      vac_ster_deworm_impute)\n",
    "if vac_ster_deworm_impute: \n",
    "    col_not_sure = ['Vaccinated', 'Sterilized', 'Dewormed']\n",
    "    for col in col_not_sure: \n",
    "        data1[col] = data1[col].map(lambda x: 2 if x == 3 else x)\n",
    "\n",
    "train1 = data2[data2.AdoptionSpeed.notna()]\n",
    "test1 = data2[data2.AdoptionSpeed.isna()].drop('AdoptionSpeed', axis=1)\n",
    "\n",
    "x = train1.drop('AdoptionSpeed', axis=1)\n",
    "y = train1['AdoptionSpeed'].astype('int')\n",
    "    \n",
    "print(data2.shape)\n",
    "#print(data2.info(memory_usage='deep'))\n",
    "\n",
    "print(dt.datetime.now(), 'finish processing data')\n",
    "\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training XGBoost or LightGBM\n",
    "Ref: https://www.kaggle.com/wrosinski/baselinemodeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y_tr distribution: Counter({4: 3357, 2: 3229, 3: 2607, 1: 2472, 0: 328})\n",
      "training LGB:\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 1.00763\tvalid_1's rmse: 1.07084\n",
      "[400]\ttraining's rmse: 0.951017\tvalid_1's rmse: 1.05154\n",
      "[600]\ttraining's rmse: 0.913425\tvalid_1's rmse: 1.04468\n",
      "[800]\ttraining's rmse: 0.885805\tvalid_1's rmse: 1.04123\n",
      "[1000]\ttraining's rmse: 0.860528\tvalid_1's rmse: 1.0395\n",
      "[1200]\ttraining's rmse: 0.836763\tvalid_1's rmse: 1.03814\n",
      "[1400]\ttraining's rmse: 0.816436\tvalid_1's rmse: 1.03727\n",
      "[1600]\ttraining's rmse: 0.795229\tvalid_1's rmse: 1.03683\n",
      "[1800]\ttraining's rmse: 0.777283\tvalid_1's rmse: 1.03665\n",
      "[2000]\ttraining's rmse: 0.760743\tvalid_1's rmse: 1.03657\n",
      "[2200]\ttraining's rmse: 0.74554\tvalid_1's rmse: 1.03622\n",
      "[2400]\ttraining's rmse: 0.726105\tvalid_1's rmse: 1.03626\n",
      "[2600]\ttraining's rmse: 0.712124\tvalid_1's rmse: 1.03616\n",
      "[2800]\ttraining's rmse: 0.698418\tvalid_1's rmse: 1.0365\n",
      "[3000]\ttraining's rmse: 0.68507\tvalid_1's rmse: 1.03665\n",
      "Early stopping, best iteration is:\n",
      "[2598]\ttraining's rmse: 0.712226\tvalid_1's rmse: 1.03612\n",
      "\n",
      "y_tr distribution: Counter({4: 3357, 2: 3229, 3: 2607, 1: 2472, 0: 328})\n",
      "training LGB:\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 1.00949\tvalid_1's rmse: 1.07052\n",
      "[400]\ttraining's rmse: 0.950425\tvalid_1's rmse: 1.05572\n",
      "[600]\ttraining's rmse: 0.917449\tvalid_1's rmse: 1.05176\n",
      "[800]\ttraining's rmse: 0.888074\tvalid_1's rmse: 1.05053\n",
      "[1000]\ttraining's rmse: 0.863219\tvalid_1's rmse: 1.04996\n",
      "[1200]\ttraining's rmse: 0.841329\tvalid_1's rmse: 1.049\n",
      "[1400]\ttraining's rmse: 0.821208\tvalid_1's rmse: 1.04821\n",
      "[1600]\ttraining's rmse: 0.798205\tvalid_1's rmse: 1.04832\n",
      "[1800]\ttraining's rmse: 0.778518\tvalid_1's rmse: 1.04858\n",
      "Early stopping, best iteration is:\n",
      "[1358]\ttraining's rmse: 0.82524\tvalid_1's rmse: 1.04814\n",
      "\n",
      "y_tr distribution: Counter({4: 3358, 2: 3230, 3: 2607, 1: 2472, 0: 328})\n",
      "training LGB:\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 1.0091\tvalid_1's rmse: 1.06097\n",
      "[400]\ttraining's rmse: 0.951558\tvalid_1's rmse: 1.04563\n",
      "[600]\ttraining's rmse: 0.915003\tvalid_1's rmse: 1.04196\n",
      "[800]\ttraining's rmse: 0.882465\tvalid_1's rmse: 1.04001\n",
      "[1000]\ttraining's rmse: 0.857471\tvalid_1's rmse: 1.03864\n",
      "[1200]\ttraining's rmse: 0.834264\tvalid_1's rmse: 1.03793\n",
      "[1400]\ttraining's rmse: 0.812939\tvalid_1's rmse: 1.03749\n",
      "[1600]\ttraining's rmse: 0.794058\tvalid_1's rmse: 1.03774\n",
      "[1800]\ttraining's rmse: 0.779247\tvalid_1's rmse: 1.03805\n",
      "Early stopping, best iteration is:\n",
      "[1397]\ttraining's rmse: 0.813323\tvalid_1's rmse: 1.03748\n",
      "\n",
      "y_tr distribution: Counter({4: 3358, 2: 3230, 3: 2607, 1: 2472, 0: 328})\n",
      "training LGB:\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 1.00772\tvalid_1's rmse: 1.06967\n",
      "[400]\ttraining's rmse: 0.950091\tvalid_1's rmse: 1.05247\n",
      "[600]\ttraining's rmse: 0.911878\tvalid_1's rmse: 1.04796\n",
      "[800]\ttraining's rmse: 0.881389\tvalid_1's rmse: 1.04587\n",
      "[1000]\ttraining's rmse: 0.856542\tvalid_1's rmse: 1.04493\n",
      "[1200]\ttraining's rmse: 0.836331\tvalid_1's rmse: 1.04459\n",
      "[1400]\ttraining's rmse: 0.816992\tvalid_1's rmse: 1.04458\n",
      "[1600]\ttraining's rmse: 0.80013\tvalid_1's rmse: 1.04482\n",
      "Early stopping, best iteration is:\n",
      "[1135]\ttraining's rmse: 0.842516\tvalid_1's rmse: 1.04447\n",
      "\n",
      "y_tr distribution: Counter({4: 3358, 2: 3230, 3: 2608, 1: 2472, 0: 328})\n",
      "training LGB:\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[200]\ttraining's rmse: 1.00684\tvalid_1's rmse: 1.07482\n",
      "[400]\ttraining's rmse: 0.951272\tvalid_1's rmse: 1.05924\n",
      "[600]\ttraining's rmse: 0.916502\tvalid_1's rmse: 1.05243\n",
      "[800]\ttraining's rmse: 0.888485\tvalid_1's rmse: 1.04984\n",
      "[1000]\ttraining's rmse: 0.862768\tvalid_1's rmse: 1.04919\n",
      "[1200]\ttraining's rmse: 0.839868\tvalid_1's rmse: 1.04844\n",
      "[1400]\ttraining's rmse: 0.82114\tvalid_1's rmse: 1.04829\n",
      "[1600]\ttraining's rmse: 0.804419\tvalid_1's rmse: 1.04837\n",
      "[1800]\ttraining's rmse: 0.783649\tvalid_1's rmse: 1.04824\n",
      "Early stopping, best iteration is:\n",
      "[1330]\ttraining's rmse: 0.82683\tvalid_1's rmse: 1.04807\n",
      "\n",
      "Valid Counts =  Counter({4: 4197, 2: 4037, 3: 3259, 1: 3090, 0: 410})\n",
      "Predicted Counts =  Counter({1.0: 3961, 3.0: 3836, 2.0: 3601, 4.0: 3595})\n",
      "Coefficients =  [ 0.45760825  2.13197758  2.48780707  2.9163767 ]\n",
      "QWK =  0.4421621979125676\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_splits = 5\n",
    "kfold = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "\n",
    "oof_train = np.zeros((x.shape[0]))\n",
    "oof_test = np.zeros((test1.shape[0], n_splits))\n",
    "\n",
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 70,\n",
    "          'max_depth': 9,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.85,\n",
    "          'feature_fraction': 0.8,\n",
    "          'min_split_gain': 0.02,\n",
    "          'min_child_samples': 150,\n",
    "          'min_child_weight': 0.02,\n",
    "          'lambda_l2': 0.0475,\n",
    "          'verbosity': -1} #, \n",
    "          #'device_type': 'gpu'}\n",
    "\n",
    "'''\n",
    "model_params = {'n_jobs': -1, 'tree_method': 'gpu_hist', 'metric': 'rmse', \n",
    "                'num_leaves': 70, 'max_depth': 9, 'gamma': 0.004, \n",
    "                'learning_rate': 0.01, 'bagging_fraction': 0.85, \n",
    "                'min_split_gain': 0.02, 'min_child_samples': 10, 'min_child_weight': 0.02, \n",
    "                'reg_alpha': 0.003, 'reg_lambda': 0.0475,  'verbosity': 200}\n",
    "'''\n",
    "\n",
    "\n",
    "# Additional parameters:\n",
    "early_stop = 500\n",
    "verbose_eval = 200\n",
    "num_rounds = 10000\n",
    "\n",
    "\n",
    "i = 0\n",
    "for train_index, valid_index in kfold.split(x, y):\n",
    "    \n",
    "    X_tr = x.iloc[train_index, :]\n",
    "    X_val = x.iloc[valid_index, :]\n",
    "    \n",
    "    y_tr = y.iloc[train_index]   \n",
    "    y_val = y.iloc[valid_index]\n",
    "    \n",
    "    print('\\ny_tr distribution: {}'.format(Counter(y_tr)))\n",
    "    \n",
    "    d_train = lgb.Dataset(X_tr, label=y_tr)\n",
    "    d_valid = lgb.Dataset(X_val, label=y_val)\n",
    "    watchlist = [d_train, d_valid]\n",
    "    \n",
    "    \n",
    "    print('training LGB:')\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      valid_sets=watchlist,\n",
    "                      verbose_eval=verbose_eval,\n",
    "                      early_stopping_rounds=early_stop)\n",
    "    '''\n",
    "    print('training XGB:')\n",
    "    model = XGBRegressor(n_estimators=num_rounds, **params, tree_method='gpu_hist') \n",
    "    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], \n",
    "          verbose=verbose_eval, eval_metric='rmse', early_stopping_rounds=early_stop)\n",
    "    '''\n",
    "    \n",
    "    val_pred = model.predict(X_val)#, num_iteration=model.best_iteration)\n",
    "    test_pred = model.predict(test1)#, num_iteration=model.best_iteration)\n",
    "    \n",
    "    oof_train[valid_index] = val_pred\n",
    "    oof_test[:, i] = test_pred\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "# Compute QWK based on OOF train predictions:\n",
    "optR = OptimizedRounder()\n",
    "optR.fit(oof_train, y)\n",
    "coefficients = optR.coefficients()\n",
    "pred_test_y_k = optR.predict(oof_train, coefficients)\n",
    "print(\"\\nValid Counts = \", Counter(y))\n",
    "print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "print(\"Coefficients = \", coefficients)\n",
    "qwk = quadratic_weighted_kappa(y, pred_test_y_k)\n",
    "print(\"QWK = \", qwk)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.distplot(oof_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PetID</th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>378fcc4fc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73c10e136</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72000c4c5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e147a4b9f</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43fbba852</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PetID  AdoptionSpeed\n",
       "0  378fcc4fc              1\n",
       "1  73c10e136              4\n",
       "2  72000c4c5              4\n",
       "3  e147a4b9f              3\n",
       "4  43fbba852              4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub['AdoptionSpeed'] = optR.predict(oof_test.mean(axis=1), coefficients)\n",
    "sample_sub['AdoptionSpeed'] = sample_sub['AdoptionSpeed'].astype('int')\n",
    "\n",
    "sample_sub.to_csv('submission.csv', index=False)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
